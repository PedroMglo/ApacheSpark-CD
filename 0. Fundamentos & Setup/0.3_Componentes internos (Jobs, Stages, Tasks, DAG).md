## 0.3. Componentes internos (Jobs, Stages, Tasks, DAG)

Esta é a parte crítica para entender performance, troubleshooting e tuning fino.

### 0.3.1. DAG (Directed Acyclic Graph)

O DAG é o **grafo das operações** que defines no teu código.

- **Nó**: uma transformação lógica (ex.: `select`, `filter`, `join`, `groupBy`).
- **Aresta**: dependência entre transformações (lineage).
- “Acyclic” = sem ciclos. Uma transformação não depende de um resultado futuro.

Processo:

1. Cada vez que aplicas uma transformação num DataFrame, **não** estás a executar nada; estás a **estender o DAG**.
2. O DAG é mantido no driver, no plano lógico.
3. Quando uma **ação** é chamada, Spark:
   - Otimiza o DAG (Catalyst).
   - Converte-o num plano físico.

O DAG é a base da **tolerância a falhas**:

- Se uma partição se perder, Spark re-executa as transformações necessárias a partir dos dados de origem, usando o lineage.

---

### 0.3.2. Jobs

Um **job** Spark é desencadeado por uma **ação** numa API de alto nível.

Exemplos de ações:

- `df.count()`
- `df.collect()`
- `df.write.parquet("...")`
- `df.show()`

Cada vez que invocas uma ação:

- O Spark verifica o DAG associado àquele DataFrame.
- Cria um **job** (visível na Spark UI).
- Um job pode ser composto por **vários stages**.

Num notebook, podes facilmente criar 10 jobs se fizeres 10 `.show()` em sequências distintas.

---

### 0.3.3. Stages

Um **stage** é um grupo de tarefas que:

- Pode ser executado **em paralelo** nas partições existentes.
- Não requer **shuffle** entre nós.

O “corte” entre stages é definido por **barreiras de shuffle**:

- Transformações que exigem redistribuição de dados por chave (por rede).
- Ex.: `groupBy`, `join` (por chave não broadcast), `distinct`, `repartition`, certos tipos de window.

pipeline simplificado:

```text
Ficheiro → map/filter/projeções → shuffle por chave → agregação por chave → escrita
           Stage 1                         Stage 2
```

Explicação:

- **Stage 1**:
  - Operações que podem ser pipelineadas sobre a leitura local de partições (sem redistribuição).
- **Stage 2**:
  - Operações que precisam dos dados reorganizados por alguma chave (ex.: `groupBy`).
  - Para isso, faz-se um **shuffle**: cada executor envia/recebe dados de outros executors.

Na Spark UI:

- Vês uma lista de stages por job.
- Cada stage tem:
  - Número de tasks.
  - Tempo total.
  - Métricas de shuffle, spill, etc.

---

### 0.3.4. Tasks

Uma **task** é a unidade mínima de execução paralela em Spark.

- Cada task trata **uma partição** de dados.
- Todas as tasks de um stage executam o **mesmo código**, mas em partições diferentes.

Por exemplo:

- Tabela com 10 ficheiros → 10 partições iniciais.
- Stage 0: 10 tasks (uma por partição).
- Se fizeres um `repartition(200)` e depois `groupBy`:
  - Stage 1 pode ter 200 tasks (output partitions do shuffle).

**Como o número de tasks é determinado**:

- Para um stage com leitura:
  - Num DataSource v2 / Parquet/ORC moderno: nº de tasks ~ nº de partições de input (blocos de ficheiros, partições lógicas).
- Para um stage de shuffle:
  - Número de tasks = número de **partições de saída do shuffle**.
  - Por default: `spark.sql.shuffle.partitions` (tipicamente 200 em versões recentes, mas verifica a tua configuração).
  - Se fizeres explicitamente `df.repartition(500)` antes de um `groupBy`, o número de partitions do shuffle será 500 (salvo overrides por AQE).

Em termos práticos:

- **Input partitions** (leitura de ficheiros):
  - Determinadas pelo formato, tamanho e split do ficheiro.
- **Shuffle partitions**:
  - Determinadas por `spark.sql.shuffle.partitions` ou por `repartition`/`coalesce` explícito, e potencialmente ajustadas pela AQE.
- **Tasks por stage**:
  - Stage de leitura: nº de tasks = nº de input partitions.
  - Stage pós-shuffle: nº de tasks = nº de shuffle partitions.

---

### 0.3.5. Como é definido o número de partitions (em detalhe)

#### 1) Partitions de leitura (source)

Para ficheiros (Parquet, ORC, etc.):

- Cada ficheiro é tipicamente **splittable**:
  - Um ficheiro grande pode ser dividido em múltiplos splits (blocos HDFS, ranges S3, etc.).
- O DataSource cria **partitions lógicas** com base em:
  - Tamanho dos ficheiros.
  - Limites de bloco (HDFS).
  - Configurações internas (por ex. `spark.sql.files.maxPartitionBytes`).

Regra prática:

- Muitos ficheiros pequenos → muitas partições pequenas (small files problem).
- Poucos ficheiros muito grandes → menos partições, mas cada task pode ficar “pesada”.

#### 2) Partitions após transformações (repartition, coalesce)

- `df.repartition(n)`:
  - Cria um shuffle.
  - Distribui dados por **n partitions**.
  - Gera **n tasks** no próximo stage.
- `df.coalesce(n)`:
  - Tenta reduzir o nº de partitions **sem shuffle** (só “juntando” partitions existentes).
  - Melhor para **diminuir** partitions, por exemplo antes de escrever.

#### 3) Partitions de shuffle (groupBy, join, etc.)

- `spark.sql.shuffle.partitions` (default 200) define o nº de partitions de saída para muitas operações de shuffle, ex.:
  - `groupBy`, `join` (non-broadcast), `distinct`, `sort` (global), etc.
- Se fizeste `repartition(50)` antes de um `groupBy`, então:
  - Input partitions para o groupBy: 50.
  - Mas o **resultado** do shuffle (hash partitioning por chave) vai ter:
    - usualmente `spark.sql.shuffle.partitions` partitions (a não ser que a engine detecte que pode manter 50, dependendo da versão/config).
- Com **Adaptive Query Execution (AQE)**:
  - O Spark pode analisar o tamanho real dos dados durante a execução e:
    - **Coalescer** partitions de shuffle (reduzir).
    - Em alguns casos, aumentar partitions em hotspots (skew handling).

---

### 0.3.6. Como as tasks são reexecutadas (fault tolerance)

Conceito de base: **lineage** e **idempotência** das transformações.

Fluxo de falha:

1. O driver envia uma task para um executor.
2. Se a task falhar (ex.: executor morre, erro de rede, OOM, etc.):
   - O driver recebe um erro/reporte do executor ou do Cluster Manager.
3. O driver **marca essa task como falhada**.
4. O scheduler tenta **reexecutar**:
   - No mesmo executor (se ainda estiver vivo) ou noutro executor.
   - Usa o lineage para recuperar a partição:
     - Se for resultado de leitura: relê o ficheiro.
     - Se for resultado de um shuffle anterior:
       - Relê as partições de shuffle necessárias dos outros executors onde o shuffle foi escrito.
5. Existe um limite de tentativas de reexecução:
   - Config: `spark.task.maxFailures` (default 4).
   - Se exceder, o **stage** é marcado como falhado.
   - Falha de um stage leva à falha do **job**, salvo casos específicos (por exemplo, alguns modes de streaming que podem permitir restart com checkpoint).

**Importante**:

- Reexecução é ao nível de **tasks**, não do job inteiro (embora o job possa falhar se demasiados erros ocorrerem).
- O Spark é tolerante a falhas de máquinas individuais, desde que a origem dos dados e o shuffle correspondente ainda estejam acessíveis.

---

### 0.3.7. Planeamento de execução (Catalyst, join strategy, partitions, etc.)

#### Plano lógico → Plano físico (Catalyst Optimizer)

O Catalyst passa por várias fases:

1. **Análise**:
   - Verifica schemas de tabelas/colunas, resolve referências.
2. **Otimizações lógicas**:
   - Push-down de filtros (`WHERE` mais cedo).
   - Pruning de colunas (remove colunas não usadas).
   - Reescrita de expressões.
3. **Planeamento físico**:
   - Decide como implementar joins (broadcast vs shuffle hash vs sort merge).
   - Decide o tipo de agregação (hash aggregate vs sort aggregate).
   - Define número de partitions de shuffle (base `spark.sql.shuffle.partitions` + heurísticas + AQE).

#### Estratégias de join/shuffle

Spark pode escolher entre várias estratégias:

- **Broadcast Hash Join**:
  - Uma das tabelas é pequena (abaixo de `spark.sql.autoBroadcastJoinThreshold`).
  - Essa tabela é **broadcast** em memória para todos os executors.
  - Evita shuffle da tabela pequena.
- **Shuffle Hash Join**:
  - Ambas as tabelas potencialmente grandes.
  - Faz shuffle por chave de join, depois hash join local.
- **Sort Merge Join**:
  - Ambas as tabelas são grandes.
  - Dados são ordenados por chave após shuffle, e o join é feito num merge ordenado.
- **AQE** pode re-escolher a estratégia durante a execução com base em estatísticas reais.

Como o número de partitions entra aqui:

- Em joins de shuffle, o nº de partitions de shuffle (normalmente `spark.sql.shuffle.partitions`) define:
  - Quantas tasks vão processar os dados joinados.
  - O paralelismo disponível para o trabalho de join.
- Se tiveres `spark.sql.shuffle.partitions = 200` e os dados são pequenos:
  - Podes ter overhead de tasks a mais.
- Se os dados são muito grandes e tens 50 partitions:
  - Podes ter tasks com demasiada carga (skew de volume, embora não necessariamente skew de chave).

É aqui que o tuning entra:

- Ajustar `spark.sql.shuffle.partitions` (global) ou usar `repartition` em pontos estratégicos da pipeline.
- Em conjunto com:
  - Tamanho do cluster (nº de executors e cores).
  - Volume de dados.
  - Distribuição das chaves.

---

### 0.3.8. Coordenação e estado “não distribuído” (Driver, metadados, broadcast, accumulators, UDFs)

#### Coordenação

O driver faz:

- **Monitorização**:
  - Recebe progresso de tasks (via RPC).
  - Atualiza Spark UI.
- **Gestão de falhas**:
  - Reexecuta tasks falhadas.
  - Se um executor morre, pede outro ao Cluster Manager, se possível.
- **Planeamento incremental (com AQE)**:
  - Analisa métricas de stages anteriores para decidir planos dos próximos.

#### Estado não distribuído (no driver ou replicado)

Alguns tipos de estado que não são “normais” dados distribuídos:

1. **Metadados**:
   - Catálogo de tabelas, schemas, estatísticas (quando usado).
   - Mantidos no metastore e em memória no driver.
2. **Broadcast variables**:
   - Permite enviar uma cópia read-only de um objeto para todos os executors.
   - Ex.: um dicionário de lookup pequeno.
   - Criadas com `spark.sparkContext.broadcast(obj)`.
   - Cada executor guarda uma cópia local em memória.
3. **Accumulators**:
   - Variáveis que acumulam valores durante as tasks.
   - Ex.: contadores, somatórios, métricas customizadas.
   - Atualizados pelas tasks; o valor consolidado é lido no driver.
   - Importante: têm semântica específica (podem ser atualizados mais que uma vez se tasks forem reexecutadas).
4. **Objetos Python usados na definição de UDFs**:
   - Quando defines uma UDF Python, o código e o contexto associado são:
     - Serializados.
     - Enviados para os executors.
   - Em cada executor, o Python worker mantém uma cópia do código da UDF.
   - Estado global em UDFs deve ser evitado ou usado com cuidado:
     - Pode ser inicializado por processo de executor.
     - Não é garantida consistência global, nem comunicação entre workers.
