{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType,\n",
    "    LongType, TimestampType, IntegerType\n",
    ")\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pathlib import Path\n",
    "from helpers import build_schema, load_schema_json, paths\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TesteLocal\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85355a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c687e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repo_root = Path.cwd()               \n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "print(schema_base_dir) ## /workspaces/ApacheSpark-CD/data/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    \"integer\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"timestamp\": TimestampType(),\n",
    "    \"float\": FloatType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "schema_paths = paths(schema_base_dir,\"file\", \"schemas*\")\n",
    "schema_json = load_schema_json(schema_paths)\n",
    "ds_list = paths(schema_base_dir,\"folder\")\n",
    "\n",
    "for ds in ds_list:\n",
    "    # print(f\"Processing {ds}\")\n",
    "    ds = Path(ds).name\n",
    "    print(f\"Processing {ds.capitalize()} data\")\n",
    "    print(f\"Processing {ds} data\")\n",
    "    \n",
    "    output_parq = (Path(f\"{schema_base_dir}_parquet\")/ds).as_posix()\n",
    "    output_delta = (Path(schema_base_dir)/ds).as_posix()\n",
    "    \n",
    "    schema_table = build_schema(ds,schema_json)\n",
    "    files=paths(f\"{schema_base_dir}/{ds}\",\"file\", \"part-*\")\n",
    "    if not files:\n",
    "        continue\n",
    "    print(files)\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(schema_table)\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .csv(files)\n",
    "    )\n",
    "     \n",
    "    df.cache()  ### df.persist(StorageLevel.MEMORY_AND_DISK) -->>> estudar\n",
    "    # print(output_dir)\n",
    "    # df.show(5)\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .format(\"parquet\")\n",
    "        .save(output_parq)\n",
    "    )\n",
    "    print(f\"{output_parq} written successfully.\")\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .format(\"delta\")\n",
    "        .save(output_delta)\n",
    "    )\n",
    "    \n",
    "    print(f\"{output_delta} written successfully.\")\n",
    "    \n",
    "    \n",
    "    df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab301dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "tabela_deltasql = \"/workspaces/ApacheSpark-CD/minha_delta_table\"\n",
    "\n",
    "# 1) Dropa a tabela do catálogo (se existir)\n",
    "spark.sql(\"DROP TABLE IF EXISTS minha_tabela_delta\")\n",
    "\n",
    "# 2) Apaga o diretório físico\n",
    "path = Path(tabela_deltasql)\n",
    "shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "# 3) Recria a tabela Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE minha_tabela_delta\n",
    "USING DELTA\n",
    "LOCATION '{tabela_deltasql}'\n",
    "AS\n",
    "SELECT 'b' as letra, 2 as numero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56367daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE minha_tabela_delta\n",
    "USING DELTA\n",
    "LOCATION '{tabela_deltasql}'\n",
    "AS\n",
    "SELECT 'b' as letra, 2 as numero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ddff1",
   "metadata": {},
   "source": [
    "# primeiras consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"stock_id\",    StringType(), True),\n",
    "    StructField(\"trans_date\",  StringType(), True),\n",
    "    StructField(\"open_price\",  FloatType(),  True),\n",
    "    StructField(\"low_price\",   FloatType(),  True),\n",
    "    StructField(\"high_price\",  FloatType(),  True),\n",
    "    StructField(\"close_price\", FloatType(),  True),\n",
    "    StructField(\"volume\",      LongType(),   True)\n",
    "])\n",
    "\n",
    "dir_data = (repo_root/\"data\"/\"nyse_all/nyse_data/*.txt.gz\").as_posix()\n",
    "df = spark.read.csv(\n",
    "    dir_data,\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = (repo_root/\"data/nyse_data_parquet\").as_posix()\n",
    "df.write.mode(\"overwrite\").parquet(dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df.dtypes)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter=(\n",
    "df\n",
    "#  df.filter(F.col(\"stock_id\" ) == \"ABRN\")\n",
    " .groupBy(\"stock_id\")\n",
    " .agg(F.count(\"*\").alias(\"num_records\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30732ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    # .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\",F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e554bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\", F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
