{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 02:08:39 WARN Utils: Your hostname, codespaces-ef10a1 resolves to a loopback address: 127.0.0.1; using 10.0.0.152 instead (on interface eth0)\n",
      "26/01/03 02:08:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/03 02:08:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.2\n",
      "local-1767406121254\n",
      "http://ee435b6a-2673-4663-964f-1c8e2569230d.internal.cloudapp.net:4040\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, LongType,TimestampType,IntegerType\n",
    "from pyspark.sql import functions as F,Window\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TesteLocal\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c687e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()               \n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "print(schema_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c9b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing departments data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/departments/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/departments written successfully.\n",
      "Processing categories data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/categories/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/categories written successfully.\n",
      "Processing orders data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/orders/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/orders written successfully.\n",
      "Processing customers data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/customers/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/customers written successfully.\n",
      "Processing products data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/products/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/products written successfully.\n",
      "Processing order_items data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/order_items/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/order_items written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 02:08:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "type_mapping = {\n",
    "    \"integer\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"timestamp\": TimestampType(),\n",
    "    \"float\": FloatType()\n",
    "}\n",
    "\n",
    "def _paths(folder_path,kind: str,pattern:str = \"*\"):\n",
    "    \n",
    "    kind = kind.lower()\n",
    "    \n",
    "    if kind not in {\"file\", \"folder\"}:\n",
    "        raise ValueError(\"Nenhum file/folder path atribuido\")\n",
    "    \n",
    "    base_path = Path(folder_path)\n",
    "    \n",
    "    if kind == \"file\":\n",
    "        path = [path.as_posix() for path in base_path.glob(pattern) if path.is_file()]\n",
    "\n",
    "    else:\n",
    "        path = [path.as_posix() for path in base_path.glob(pattern) if path.is_dir()]\n",
    "    \n",
    "    if not path:\n",
    "        warnings.warn(f\"[WARN] Nenhum {kind} encontrado em {base_path} com pattern='{pattern}'\")\n",
    "    return path\n",
    "\n",
    "def _load_schema_json(schema_paths) -> dict:\n",
    "\n",
    "    schema_path = schema_paths[0] if isinstance(schema_paths, (list, tuple)) else schema_paths\n",
    "    \n",
    "    with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _build_schema(table_name: str, schema_json: dict) -> StructType:\n",
    "    if table_name not in schema_json:\n",
    "        raise KeyError(f\"Tabela {table_name} n√£o encontrada no JSON de schema\")\n",
    "    fields = sorted(schema_json[table_name], key= lambda col: col[\"column_position\"], reverse = False)\n",
    "    \n",
    "    return StructType(\n",
    "       [ StructField(\n",
    "            field[\"column_name\"]\n",
    "            ,type_mapping.get(field[\"data_type\"].lower(),StringType())\n",
    "            ,True\n",
    "        )\n",
    "        for field in fields]\n",
    "    )\n",
    "\n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "output_dir_parq = f\"{schema_base_dir}_parquet\"\n",
    "schema_paths = _paths(schema_base_dir,\"file\", \"schemas*\")\n",
    "schema_json = _load_schema_json(schema_paths)\n",
    "ds_list = _paths(schema_base_dir,\"folder\")\n",
    "\n",
    "for ds in ds_list:\n",
    "    ds = Path(ds).name\n",
    "    print(f\"Processing {ds} data\")\n",
    "    \n",
    "    schema_table = _build_schema(ds,schema_json)\n",
    "    files=_paths(f\"{schema_base_dir}/{ds}\",\"file\", \"part-*\")\n",
    "    if not files:\n",
    "        continue\n",
    "    print(files)\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(schema_table)\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .csv(files)\n",
    "    )\n",
    "    \n",
    "    output_dir = (Path(output_dir_parq)/ds).as_posix()\n",
    "    # print(output_dir)\n",
    "    # df.show(5)\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .parquet(output_dir)\n",
    "    )\n",
    "    print(f\"{output_dir} written successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ddff1",
   "metadata": {},
   "source": [
    "# primeiras consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df41c4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 02:16:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20160101, 41.81, 41.81, 41.81, 41.81, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2016.txt.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+---------+----------+-----------+------+\n",
      "|stock_id|trans_date|open_price|low_price|high_price|close_price|volume|\n",
      "+--------+----------+----------+---------+----------+-----------+------+\n",
      "|      AA|  20160101|     29.61|    29.61|     29.61|      29.61|     0|\n",
      "|     AAC|  20160101|     19.06|    19.06|     19.06|      19.06|     0|\n",
      "|     AAN|  20160101|     22.39|    22.39|     22.39|      22.39|     0|\n",
      "|     AAP|  20160101|    150.51|   150.51|    150.51|     150.51|     0|\n",
      "|     AAT|  20160101|     38.35|    38.35|     38.35|      38.35|     0|\n",
      "+--------+----------+----------+---------+----------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"stock_id\",    StringType(), True),\n",
    "    StructField(\"trans_date\",  StringType(), True),\n",
    "    StructField(\"open_price\",  FloatType(),  True),\n",
    "    StructField(\"low_price\",   FloatType(),  True),\n",
    "    StructField(\"high_price\",  FloatType(),  True),\n",
    "    StructField(\"close_price\", FloatType(),  True),\n",
    "    StructField(\"volume\",      LongType(),   True)\n",
    "])\n",
    "\n",
    "dir_data = (repo_root/\"data\"/\"nyse_all/nyse_data/*.txt.gz\").as_posix()\n",
    "df = spark.read.csv(\n",
    "    dir_data,\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e515acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 02:17:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20080101, 36.74, 36.74, 36.74, 36.74, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2008.txt.gz\n",
      "26/01/03 02:17:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20160101, 41.81, 41.81, 41.81, 41.81, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2016.txt.gz\n",
      "26/01/03 02:17:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20070101, 34.85, 34.85, 34.85, 34.85, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2007.txt.gz\n",
      "26/01/03 02:17:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20150101, 40.94, 40.94, 40.94, 40.94, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2015.txt.gz\n",
      "26/01/03 02:17:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20060102, 33.29, 33.29, 33.29, 33.29, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2006.txt.gz\n",
      "26/01/03 02:17:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20050103, 24.1, 24.18, 23.5, 23.88, 2416900\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2005.txt.gz\n",
      "26/01/03 02:17:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20140101, 57.19, 57.19, 57.19, 57.19, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2014.txt.gz\n",
      "26/01/03 02:17:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20040101, 29.24, 29.24, 29.24, 29.24, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2004.txt.gz\n",
      "26/01/03 02:17:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20030101, 17.96, 17.96, 17.96, 17.96, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2003.txt.gz\n",
      "26/01/03 02:17:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20130101, 40.94, 40.94, 40.94, 40.94, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2013.txt.gz\n",
      "26/01/03 02:17:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20020101, 28.51, 28.51, 28.51, 28.51, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2002.txt.gz\n",
      "26/01/03 02:17:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20010101, 54.75, 54.75, 54.75, 54.75, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2001.txt.gz\n",
      "26/01/03 02:17:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20120102, 34.93, 34.93, 34.93, 34.93, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2012.txt.gz\n",
      "26/01/03 02:18:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20000103, 78.75, 78.94, 67.38, 72, 3343700\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2000.txt.gz\n",
      "26/01/03 02:18:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20110103, 41.56, 42.14, 41.41, 41.88, 3572300\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2011.txt.gz\n",
      "26/01/03 02:18:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: AA, 19990101, 55.92, 55.92, 55.92, 55.92, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: AA\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_1999.txt.gz\n",
      "26/01/03 02:18:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: AA, 19980101, 52.77, 52.77, 52.77, 52.77, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: AA\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_1998.txt.gz\n",
      "26/01/03 02:18:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20100101, 31.07, 31.07, 31.07, 31.07, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2010.txt.gz\n",
      "26/01/03 02:18:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: AA, 19970101, 47.82, 47.82, 47.82, 47.82, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: AA\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_1997.txt.gz\n",
      "26/01/03 02:18:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20090101, 15.63, 15.63, 15.63, 15.63, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2009.txt.gz\n",
      "26/01/03 02:18:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: A, 20170102, 45.56, 45.56, 45.56, 45.56, 0\n",
      " Schema: stock_id, trans_date, open_price, low_price, high_price, close_price, volume\n",
      "Expected: stock_id but found: A\n",
      "CSV file: file:///workspaces/ApacheSpark-CD/data/nyse_all/nyse_data/NYSE_2017.txt.gz\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dir_data = (repo_root/\"data/nyse_data_parquet\").as_posix()\n",
    "df.write.mode(\"overwrite\").parquet(dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df.dtypes)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter=(\n",
    "df\n",
    "#  df.filter(F.col(\"stock_id\" ) == \"ABRN\")\n",
    " .groupBy(\"stock_id\")\n",
    " .agg(F.count(\"*\").alias(\"num_records\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30732ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    # .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\",F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e554bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\", F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
