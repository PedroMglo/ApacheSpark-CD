{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# raiz do projeto no devcontainer\n",
    "repo_root = Path(\"/workspaces/ApacheSpark-CD\")\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "from python.helpers import build_schema, load_schema_json, paths  \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType,\n",
    "    LongType, TimestampType, IntegerType\n",
    ")\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a26e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xms512m -Xmx4g\n",
      "Picked up _JAVA_OPTIONS: -Xms512m -Xmx4g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/workspaces/ApacheSpark-CD/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fc54888e-3ab3-4f5e-9552-cf0f548eb805;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.postgresql#postgresql;42.7.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.41.0 in central\n",
      ":: resolution report :: resolve 378ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.41.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fc54888e-3ab3-4f5e-9552-cf0f548eb805\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n",
      "26/01/07 10:05:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/07 10:05:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão iniciada com pacotes Delta + Postgres!\n",
      "Spark version: 3.5.2\n",
      "local-1767780356250\n",
      "http://14b23abd838f:4041\n"
     ]
    }
   ],
   "source": [
    "postgres_package = \"org.postgresql:postgresql:42.7.1\"\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TesteLocal\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "# Adicionamos o pacote do Postgres através do extra_packages\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=[postgres_package]).getOrCreate()\n",
    "\n",
    "print(\"Sessão iniciada com pacotes Delta + Postgres!\")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16338295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentando conectar em: db...\n",
      "Sucesso! Schema da tabela orders:\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "Conexão Spark -> Postgres realizada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "db_host = os.getenv(\"POSTGRES_HOST\", \"db\") \n",
    "\n",
    "print(f\"Tentando conectar em: {db_host}...\")\n",
    "\n",
    "db_url = f\"jdbc:postgresql://{db_host}:5432/retail_db\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Lendo uma tabela do Postgres\n",
    "try:\n",
    "    df_orders = spark.read.jdbc(url=db_url, table=\"orders\", properties=db_properties)\n",
    "    print(\"Sucesso! Schema da tabela orders:\")\n",
    "    df_orders.printSchema()\n",
    "    \n",
    "    # print(\"A mostrar 5 registos:\")\n",
    "    # df_orders.show(5)\n",
    "    print(\"Conexão Spark -> Postgres realizada com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85355a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c687e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path(\"/workspaces/ApacheSpark-CD\")               \n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "# print(schema_base_dir) ## /workspaces/ApacheSpark-CD/data/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b560f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    \"integer\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"timestamp\": TimestampType(),\n",
    "    \"float\": FloatType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2c9b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db\n",
      "Processing Departments data\n",
      "Processing departments data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/departments/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/departments written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/07 10:06:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/departments written successfully.\n",
      "Processing Categories data\n",
      "Processing categories data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/categories/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/categories written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/categories written successfully.\n",
      "Processing Orders data\n",
      "Processing orders data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/orders/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/orders written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/orders written successfully.\n",
      "Processing Customers data\n",
      "Processing customers data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/customers/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/customers written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/customers written successfully.\n",
      "Processing Products data\n",
      "Processing products data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/products/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/products written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/products written successfully.\n",
      "Processing Order_items data\n",
      "Processing order_items data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/order_items/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/order_items written successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/order_items written successfully.\n"
     ]
    }
   ],
   "source": [
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "schema_paths = paths(schema_base_dir,\"file\", \"schemas.json\")\n",
    "print(schema_base_dir)\n",
    "schema_json = load_schema_json(schema_paths)\n",
    "ds_list = paths(schema_base_dir,\"folder\")\n",
    "\n",
    "for ds in ds_list:\n",
    "    # print(f\"Processing {ds}\")\n",
    "    ds = Path(ds).name\n",
    "    print(f\"Processing {ds.capitalize()} data\")\n",
    "    print(f\"Processing {ds} data\")\n",
    "    \n",
    "    output_parq = (Path(f\"{schema_base_dir}_parquet\")/ds).as_posix()\n",
    "    output_delta = (Path(f\"{schema_base_dir}_delta\")/ds).as_posix()\n",
    "    \n",
    "    schema_table = build_schema(ds,schema_json,type_mapping)\n",
    "    files=paths(f\"{schema_base_dir}/{ds}\",\"file\", \"part-*\")\n",
    "    if not files:\n",
    "        continue\n",
    "    print(files)\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(schema_table)\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .csv(files)\n",
    "    )\n",
    "     \n",
    "    df.cache()  ### df.persist(StorageLevel.MEMORY_AND_DISK) -->>> estudar\n",
    "    # print(output_dir)\n",
    "    # df.show(5)\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .format(\"parquet\")\n",
    "        .save(output_parq)\n",
    "    )\n",
    "    print(f\"{output_parq} written successfully.\")\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .format(\"delta\")\n",
    "        .save(output_delta)\n",
    "    )\n",
    "    \n",
    "    print(f\"{output_delta} written successfully.\")\n",
    "    \n",
    "    \n",
    "    df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab301dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "tabela_deltasql = \"/workspaces/ApacheSpark-CD/minha_delta_table\"\n",
    "\n",
    "# 1) Dropa a tabela do catálogo (se existir)\n",
    "spark.sql(\"DROP TABLE IF EXISTS minha_tabela_delta\")\n",
    "\n",
    "# 2) Apaga o diretório físico\n",
    "path = Path(tabela_deltasql)\n",
    "shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "# 3) Recria a tabela Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE minha_tabela_delta\n",
    "USING DELTA\n",
    "LOCATION '{tabela_deltasql}'\n",
    "AS\n",
    "SELECT 'b' as letra, 2 as numero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56367daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE minha_tabela_delta\n",
    "USING DELTA\n",
    "LOCATION '{tabela_deltasql}'\n",
    "AS\n",
    "SELECT 'b' as letra, 2 as numero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ddff1",
   "metadata": {},
   "source": [
    "# primeiras consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"stock_id\",    StringType(), True),\n",
    "    StructField(\"trans_date\",  StringType(), True),\n",
    "    StructField(\"open_price\",  FloatType(),  True),\n",
    "    StructField(\"low_price\",   FloatType(),  True),\n",
    "    StructField(\"high_price\",  FloatType(),  True),\n",
    "    StructField(\"close_price\", FloatType(),  True),\n",
    "    StructField(\"volume\",      LongType(),   True)\n",
    "])\n",
    "\n",
    "dir_data = (repo_root/\"data\"/\"nyse_all/nyse_data/*.txt.gz\").as_posix()\n",
    "df = spark.read.csv(\n",
    "    dir_data,\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = (repo_root/\"data/nyse_data_parquet\").as_posix()\n",
    "df.write.mode(\"overwrite\").parquet(dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df.dtypes)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter=(\n",
    "df\n",
    "#  df.filter(F.col(\"stock_id\" ) == \"ABRN\")\n",
    " .groupBy(\"stock_id\")\n",
    " .agg(F.count(\"*\").alias(\"num_records\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30732ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    # .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\",F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e554bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\", F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
