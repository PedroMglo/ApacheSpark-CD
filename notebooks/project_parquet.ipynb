{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a079bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "repo_root = Path(\"/workspaces/ApacheSpark-CD\")\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "from python.helpers import build_schema, load_schema_json, paths  \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType,\n",
    "    LongType, TimestampType, IntegerType\n",
    ")\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta import configure_spark_with_delta_pip  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a26e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Xms512m -Xmx4g\n",
      "Picked up _JAVA_OPTIONS: -Xms512m -Xmx4g\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark-4.0.1-bin-hadoop3/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/vscode/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2a3f4ecd-ceaf-4417-95f0-58a2657dc119;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.1 in central\n",
      "\tfound io.delta#delta-storage;4.0.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound org.postgresql#postgresql;42.7.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.41.0 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.1/delta-spark_2.13-4.0.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.13;4.0.1!delta-spark_2.13.jar (419ms)\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.1/postgresql-42.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.7.1!postgresql.jar (84ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.1/delta-storage-4.0.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;4.0.1!delta-storage.jar (55ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.13.1!antlr4-runtime.jar (59ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.41.0/checker-qual-3.41.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.41.0!checker-qual.jar (52ms)\n",
      ":: resolution report :: resolve 3086ms :: artifacts dl 675ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.1 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.41.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2a3f4ecd-ceaf-4417-95f0-58a2657dc119\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (9227kB/11ms)\n",
      "26/01/12 21:57:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão iniciada com pacotes Delta + Postgres!\n",
      "Spark version: 4.0.1\n",
      "local-1768255049382\n",
      "http://0.0.0.0:4040\n"
     ]
    }
   ],
   "source": [
    "postgres_package = \"org.postgresql:postgresql:42.7.1\"\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TesteLocal\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "# Adicionamos o pacote do Postgres através do extra_packages\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=[postgres_package]).getOrCreate()\n",
    "\n",
    "print(\"Sessão iniciada com pacotes Delta + Postgres!\")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16338295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentando conectar em: postgres_db...\n",
      "Sucesso! Schema da tabela orders:\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|                1|         CLOSED|\n",
      "|       2|2013-07-26 00:00:00|                1|PENDING_PAYMENT|\n",
      "|       3|2013-07-27 00:00:00|                1|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "\n",
      "Conexão Spark -> Postgres realizada com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "db_host = os.environ.get(\"POSTGRES_HOST\") \n",
    "\n",
    "print(f\"Tentando conectar em: {db_host}...\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{db_host}:5432/retail_db\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": os.environ.get(\"POSTGRES_USER\"),\n",
    "    \"password\": os.environ[\"POSTGRES_PASSWORD\"],   # falha se não existir (bom para detectar)\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "# Lendo uma tabela do Postgres\n",
    "try:\n",
    "    df_orders = spark.read.jdbc(url=jdbc_url, table=\"orders\", properties=db_properties)\n",
    "    print(\"Sucesso! Schema da tabela orders:\")\n",
    "    df_orders.printSchema()\n",
    "    \n",
    "    # print(\"A mostrar 5 registos:\")\n",
    "    df_orders.show(5)\n",
    "    print(\"Conexão Spark -> Postgres realizada com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao conectar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85355a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c687e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path(\"/workspaces/ApacheSpark-CD\")               \n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "# print(schema_base_dir) ## /workspaces/ApacheSpark-CD/data/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b560f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    \"integer\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"timestamp\": TimestampType(),\n",
    "    \"float\": FloatType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2c9b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db\n",
      "Processing Orders data\n",
      "Processing orders data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/orders/part-00000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/orders written successfully.\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/orders written successfully.\n",
      "Processing Products data\n",
      "Processing products data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/products/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/products written successfully.\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/products written successfully.\n",
      "Processing Order_items data\n",
      "Processing order_items data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/order_items/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/order_items written successfully.\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/order_items written successfully.\n",
      "Processing Categories data\n",
      "Processing categories data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/categories/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/categories written successfully.\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/categories written successfully.\n",
      "Processing Departments data\n",
      "Processing departments data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/departments/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/departments written successfully.\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/departments written successfully.\n",
      "Processing Customers data\n",
      "Processing customers data\n",
      "['/workspaces/ApacheSpark-CD/data/retail_db/customers/part-00000']\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_parquet/customers written successfully.\n",
      "/workspaces/ApacheSpark-CD/data/retail_db_delta/customers written successfully.\n"
     ]
    }
   ],
   "source": [
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "schema_paths = paths(schema_base_dir,\"file\", \"schemas.json\")\n",
    "print(schema_base_dir)\n",
    "schema_json = load_schema_json(schema_paths)\n",
    "ds_list = paths(schema_base_dir,\"folder\")\n",
    "\n",
    "for ds in ds_list:\n",
    "    # print(f\"Processing {ds}\")\n",
    "    ds = Path(ds).name\n",
    "    print(f\"Processing {ds.capitalize()} data\")\n",
    "    print(f\"Processing {ds} data\")\n",
    "    \n",
    "    output_parq = (Path(f\"{schema_base_dir}_parquet\")/ds).as_posix()\n",
    "    output_delta = (Path(f\"{schema_base_dir}_delta\")/ds).as_posix()\n",
    "    \n",
    "    schema_table = build_schema(ds,schema_json,type_mapping)\n",
    "    files=paths(f\"{schema_base_dir}/{ds}\",\"file\", \"part-*\")\n",
    "    if not files:\n",
    "        continue\n",
    "    print(files)\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(schema_table)\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .csv(files)\n",
    "    )\n",
    "     \n",
    "    df.cache()  ### df.persist(StorageLevel.MEMORY_AND_DISK) -->>> estudar\n",
    "    # print(output_dir)\n",
    "    # df.show(5)\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .format(\"parquet\")\n",
    "        .save(output_parq)\n",
    "    )\n",
    "    print(f\"{output_parq} written successfully.\")\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"overwrite\")      # ou \"append\"\n",
    "        .format(\"delta\")\n",
    "        .save(output_delta)\n",
    "    )\n",
    "    \n",
    "    print(f\"{output_delta} written successfully.\")\n",
    "    \n",
    "    \n",
    "    df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab301dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "tabela_deltasql = \"/workspaces/ApacheSpark-CD/minha_delta_table\"\n",
    "\n",
    "# 1) Dropa a tabela do catálogo (se existir)\n",
    "spark.sql(\"DROP TABLE IF EXISTS minha_tabela_delta\")\n",
    "\n",
    "# 2) Apaga o diretório físico\n",
    "path = Path(tabela_deltasql)\n",
    "shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "# 3) Recria a tabela Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE minha_tabela_delta\n",
    "USING DELTA\n",
    "LOCATION '{tabela_deltasql}'\n",
    "AS\n",
    "SELECT 'b' as letra, 2 as numero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56367daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `default`.`minha_tabela_delta` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      2\u001b[39m \u001b[33;43mCREATE TABLE minha_tabela_delta\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[33;43mUSING DELTA\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[33;43mLOCATION \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtabela_deltasql\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43mAS\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43mSELECT \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m as letra, 2 as numero\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ApacheSpark-CD/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ApacheSpark-CD/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ApacheSpark-CD/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `default`.`minha_tabela_delta` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects."
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE minha_tabela_delta\n",
    "USING DELTA\n",
    "LOCATION '{tabela_deltasql}'\n",
    "AS\n",
    "SELECT 'b' as letra, 2 as numero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ddff1",
   "metadata": {},
   "source": [
    "# primeiras consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"stock_id\",    StringType(), True),\n",
    "    StructField(\"trans_date\",  StringType(), True),\n",
    "    StructField(\"open_price\",  FloatType(),  True),\n",
    "    StructField(\"low_price\",   FloatType(),  True),\n",
    "    StructField(\"high_price\",  FloatType(),  True),\n",
    "    StructField(\"close_price\", FloatType(),  True),\n",
    "    StructField(\"volume\",      LongType(),   True)\n",
    "])\n",
    "\n",
    "dir_data = (repo_root/\"data\"/\"nyse_all/nyse_data/*.txt.gz\").as_posix()\n",
    "df = spark.read.csv(\n",
    "    dir_data,\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e515acf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dir_data = (repo_root/\u001b[33m\"\u001b[39m\u001b[33mdata/nyse_data_parquet\u001b[39m\u001b[33m\"\u001b[39m).as_posix()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m.write.mode(\u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m).parquet(dir_data)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "dir_data = (repo_root/\"data/nyse_data_parquet\").as_posix()\n",
    "df.write.mode(\"overwrite\").parquet(dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df.dtypes)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter=(\n",
    "df\n",
    "#  df.filter(F.col(\"stock_id\" ) == \"ABRN\")\n",
    " .groupBy(\"stock_id\")\n",
    " .agg(F.count(\"*\").alias(\"num_records\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30732ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m w = Window.partitionBy(\u001b[33m\"\u001b[39m\u001b[33mstock_id\u001b[39m\u001b[33m\"\u001b[39m).orderBy(F.desc(\u001b[33m\"\u001b[39m\u001b[33mtrans_date\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m count_filter = (\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mdf\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# .filter(F.col(\"stock_id\") == \"ABRN\")\u001b[39;00m\n\u001b[32m      6\u001b[39m     .select(\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstock_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrans_date\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclose_price\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     11\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mnum_records\u001b[39m\u001b[33m\"\u001b[39m,F.row_number().over(w))\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    # .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\",F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e554bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m w = Window.partitionBy(\u001b[33m\"\u001b[39m\u001b[33mstock_id\u001b[39m\u001b[33m\"\u001b[39m).orderBy(F.desc(\u001b[33m\"\u001b[39m\u001b[33mtrans_date\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m count_filter = (\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mdf\u001b[49m\n\u001b[32m      5\u001b[39m     .filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mstock_id\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mABRN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     .select(\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstock_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrans_date\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclose_price\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     11\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mnum_records\u001b[39m\u001b[33m\"\u001b[39m, F.row_number().over(w))\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\", F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
