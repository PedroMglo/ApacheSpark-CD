{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e421fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()               \n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "print(schema_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.2\n",
      "local-1767353222937\n",
      "http://c21ccc96-d470-4a63-ab62-9cdb0f48db98.internal.cloudapp.net:4040\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, LongType,TimestampType,IntegerType\n",
    "from pyspark.sql import functions as F,Window\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "repo_root = Path.cwd()  \n",
    "# java_home = r\"C:\\Users\\pedro.lourenco\\AppData\\Local\\Programs\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\"\n",
    "# os.environ[\"JAVA_HOME\"] = java_home\n",
    "# os.environ[\"PATH\"] = str(Path(java_home) / \"bin\") + \";\" + os.environ[\"PATH\"]\n",
    "\n",
    "# # ✅ força SPARK_HOME para o pyspark do venv\n",
    "# pyspark_dir = Path(pyspark.__file__).resolve().parent  # ...\\site-packages\\pyspark\n",
    "# os.environ[\"SPARK_HOME\"] = str(pyspark_dir)\n",
    "\n",
    "# print(\"SPARK_HOME:\", os.environ[\"SPARK_HOME\"])\n",
    "# print(\"spark-submit.cmd:\", (pyspark_dir / \"bin\" / \"spark-submit.cmd\").exists())\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TesteLocal\")\n",
    "    .master(\"local[*]\")\n",
    "    # # ✅ desativa NativeIO no Hadoop (evita o access0)\n",
    "    # .config(\"spark.hadoop.io.nativeio.native.disable\", \"true\")\n",
    "    # # ✅ também ajuda em Windows\n",
    "    # .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\")\n",
    "    # .config(\"spark.hadoop.fs.AbstractFileSystem.file.impl\", \"org.apache.hadoop.fs.local.LocalFs\")\n",
    "    # # (opcional) evita certas otimizações locais\n",
    "    # .config(\"spark.hadoop.dfs.client.read.shortcircuit\", \"false\")\n",
    "    # .config(\"spark.hadoop.dfs.client.use.legacy.blockreader.local\", \"false\")\n",
    "    # .config(\"spark.pyspark.python\", sys.executable)\n",
    "    # .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "    # .config(\"spark.local.dir\", r\"C:\\temp\\spark_local\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"stock_id\",    StringType(), True),\n",
    "    StructField(\"trans_date\",  StringType(), True),\n",
    "    StructField(\"open_price\",  FloatType(),  True),\n",
    "    StructField(\"low_price\",   FloatType(),  True),\n",
    "    StructField(\"high_price\",  FloatType(),  True),\n",
    "    StructField(\"close_price\", FloatType(),  True),\n",
    "    StructField(\"volume\",      LongType(),   True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\n",
    "    _files_path(r\"C:\\Users\\pedro.lourenco\\GCP\\PySpark\\data\\nyse_all\\nyse_data\",\"*.txt.gz\"),\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25351121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"overwrite\").parquet(r\"C:\\Users\\pedro.lourenco\\GCP\\PySpark\\data\\nyse_all\\nyse_data\\nyse_data_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b505f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[stock_id: string, trans_date: string, open_price: float, low_price: float, high_price: float, close_price: float, volume: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472f27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stock_id': 'string',\n",
       " 'trans_date': 'string',\n",
       " 'open_price': 'float',\n",
       " 'low_price': 'float',\n",
       " 'high_price': 'float',\n",
       " 'close_price': 'float',\n",
       " 'volume': 'bigint'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      " |-- open_price: float (nullable = true)\n",
      " |-- low_price: float (nullable = true)\n",
      " |-- high_price: float (nullable = true)\n",
      " |-- close_price: float (nullable = true)\n",
      " |-- volume: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599523e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9384718"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter=(\n",
    "df\n",
    "#  df.filter(F.col(\"stock_id\" ) == \"ABRN\")\n",
    " .groupBy(\"stock_id\")\n",
    " .agg(F.count(\"*\").alias(\"num_records\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|stock_id|num_records|\n",
      "+--------+-----------+\n",
      "|     BOX|       1603|\n",
      "|     CCK|       5230|\n",
      "|    CNXC|        405|\n",
      "|     CRS|       5230|\n",
      "|     EFR|       3431|\n",
      "|     FMY|       3039|\n",
      "|     GIS|       5230|\n",
      "|     IHD|       1495|\n",
      "|       K|       5230|\n",
      "|     LEN|       5230|\n",
      "|     MHF|       5230|\n",
      "|     PKE|       5230|\n",
      "|     TLI|       4738|\n",
      "|     TNP|       3881|\n",
      "|     TYG|       3365|\n",
      "|    SPGI|        190|\n",
      "|     AAT|       1569|\n",
      "|     AIV|       5230|\n",
      "|     AVX|       5230|\n",
      "|     AVY|       5230|\n",
      "+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53396291",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    # .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\",F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e781564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+-----------+\n",
      "|stock_id|trans_date|close_price|num_records|\n",
      "+--------+----------+-----------+-----------+\n",
      "|      AA|  20170117|      32.64|          1|\n",
      "|      AA|  20170116|      33.01|          2|\n",
      "|      AA|  20170113|      33.01|          3|\n",
      "|      AA|  20170112|      33.04|          4|\n",
      "|      AA|  20170111|      31.97|          5|\n",
      "|      AA|  20170110|      30.98|          6|\n",
      "|      AA|  20170109|      29.48|          7|\n",
      "|      AA|  20170106|      30.68|          8|\n",
      "|      AA|  20170105|      30.65|          9|\n",
      "|      AA|  20170104|      30.26|         10|\n",
      "|      AA|  20170103|      28.83|         11|\n",
      "|      AA|  20170102|      28.08|         12|\n",
      "|      AA|  20161230|      28.08|         13|\n",
      "|      AA|  20161229|      28.89|         14|\n",
      "|      AA|  20161228|      29.43|         15|\n",
      "|      AA|  20161227|      29.65|         16|\n",
      "|      AA|  20161226|      29.71|         17|\n",
      "|      AA|  20161223|      29.71|         18|\n",
      "|      AA|  20161222|      29.75|         19|\n",
      "|      AA|  20161221|      30.43|         20|\n",
      "+--------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499eee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"stock_id\").orderBy(F.desc(\"trans_date\"))\n",
    "\n",
    "count_filter = (\n",
    "    df\n",
    "    .filter(F.col(\"stock_id\") == \"ABRN\")\n",
    "    .select(\n",
    "        \"stock_id\",\n",
    "        \"trans_date\",\n",
    "        \"close_price\"\n",
    "    )\n",
    "    .withColumn(\"num_records\", F.row_number().over(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17eb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filter.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cce70",
   "metadata": {},
   "source": [
    "# Json Load FIles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b33d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4366/3148205491.py:27: UserWarning: [WARN] Nenhum file encontrado em /workspaces/Spark-CodeSpace/data/retail_db com pattern='schemas*'\n",
      "  warnings.warn(f\"[WARN] Nenhum {kind} encontrado em {base_path} com pattern='{pattern}'\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m output_dir_parq = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_base_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m schema_paths = _paths(schema_base_dir,\u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mschemas*\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m schema_json = \u001b[43m_load_schema_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m ds_list = _paths(schema_base_dir,\u001b[33m\"\u001b[39m\u001b[33mfolder\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m ds_list:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36m_load_schema_json\u001b[39m\u001b[34m(schema_paths)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_schema_json\u001b[39m(schema_paths) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     schema_path = \u001b[43mschema_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema_paths, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m schema_paths\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(schema_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.load(f)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "type_mapping = {\n",
    "    \"integer\": IntegerType(),\n",
    "    \"string\": StringType(),\n",
    "    \"timestamp\": TimestampType(),\n",
    "    \"float\": FloatType()\n",
    "}\n",
    "\n",
    "def _paths(folder_path,kind: str,pattern:str = \"*\"):\n",
    "    \n",
    "    kind = kind.lower()\n",
    "    \n",
    "    if kind not in {\"file\", \"folder\"}:\n",
    "        raise ValueError(\"Nenhum file/folder path atribuido\")\n",
    "    \n",
    "    base_path = Path(folder_path)\n",
    "    \n",
    "    if kind == \"file\":\n",
    "        path = [path.as_posix() for path in base_path.glob(pattern) if path.is_file()]\n",
    "\n",
    "    else:\n",
    "        path = [path.as_posix() for path in base_path.glob(pattern) if path.is_dir()]\n",
    "    \n",
    "    if not path:\n",
    "        warnings.warn(f\"[WARN] Nenhum {kind} encontrado em {base_path} com pattern='{pattern}'\")\n",
    "    return path\n",
    "\n",
    "def _load_schema_json(schema_paths) -> dict:\n",
    "\n",
    "    schema_path = schema_paths[0] if isinstance(schema_paths, (list, tuple)) else schema_paths\n",
    "    \n",
    "    with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _build_schema(table_name: str, schema_json: dict) -> StructType:\n",
    "    if table_name not in schema_json:\n",
    "        raise KeyError(f\"Tabela {table_name} não encontrada no JSON de schema\")\n",
    "    fields = sorted(schema_json[table_name], key= lambda col: col[\"column_position\"], reverse = False)\n",
    "    \n",
    "    return StructType(\n",
    "       [ StructField(\n",
    "            field[\"column_name\"]\n",
    "            ,type_mapping.get(field[\"data_type\"].lower(),StringType())\n",
    "            ,True\n",
    "        )\n",
    "        for field in fields]\n",
    "    )\n",
    "\n",
    "schema_base_dir = (repo_root/\"data\"/\"retail_db\").as_posix()\n",
    "output_dir_parq = f\"{schema_base_dir}_parquet\"\n",
    "schema_paths = _paths(schema_base_dir,\"file\", \"schemas*\")\n",
    "schema_json = _load_schema_json(schema_paths)\n",
    "ds_list = _paths(schema_base_dir,\"folder\")\n",
    "\n",
    "for ds in ds_list:\n",
    "    ds = Path(ds).name\n",
    "    print(f\"Processing {ds} data\")\n",
    "    \n",
    "    schema_table = _build_schema(ds,schema_json)\n",
    "    files=_paths(f\"{schema_base_dir}/{ds}\",\"file\", \"part-*\")\n",
    "    if not files:\n",
    "        continue\n",
    "    print(files)\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .schema(schema_table)\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .csv(files)\n",
    "    )\n",
    "    \n",
    "    output_dir = (Path(output_dir_parq)/ds).as_posix()\n",
    "    print(output_dir)\n",
    "    df.show(5)\n",
    "    # (\n",
    "    #     df.write\n",
    "    #     .mode(\"overwrite\")      # ou \"append\"\n",
    "    #     .parquet(output_dir)\n",
    "    # )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399374b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_group = (\n",
    "    df\n",
    "    .groupBy(\"order_status\")\n",
    "    .agg(F.count(\"*\").alias(\"order_count\"))\n",
    "    .orderBy(F.col(\"order_count\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacbe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['order_count DESC NULLS LAST], true\n",
      "+- Aggregate [order_status#762], [order_status#762, count(1) AS order_count#772L]\n",
      "   +- Relation [order_id#759,order_date#760,order_customer_id#761,order_status#762] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_status: string, order_count: bigint\n",
      "Sort [order_count#772L DESC NULLS LAST], true\n",
      "+- Aggregate [order_status#762], [order_status#762, count(1) AS order_count#772L]\n",
      "   +- Relation [order_id#759,order_date#760,order_customer_id#761,order_status#762] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [order_count#772L DESC NULLS LAST], true\n",
      "+- Aggregate [order_status#762], [order_status#762, count(1) AS order_count#772L]\n",
      "   +- Project [order_status#762]\n",
      "      +- Relation [order_id#759,order_date#760,order_customer_id#761,order_status#762] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [order_count#772L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(order_count#772L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=231]\n",
      "      +- HashAggregate(keys=[order_status#762], functions=[count(1)], output=[order_status#762, order_count#772L])\n",
      "         +- Exchange hashpartitioning(order_status#762, 200), ENSURE_REQUIREMENTS, [plan_id=228]\n",
      "            +- HashAggregate(keys=[order_status#762], functions=[partial_count(1)], output=[order_status#762, count#777L])\n",
      "               +- FileScan csv [order_status#762] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/pedro.lourenco/GCP/PySpark/data/retail_db/orders/part-0..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_status:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "de_group.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859772f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+---------------------+---------------+\n",
      "|order_id|order_date           |order_customer_id    |order_status   |\n",
      "+--------+---------------------+---------------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|+11599-01-01 00:00:00|CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|NULL                 |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|+12111-01-01 00:00:00|COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827-01-01 00:00:00  |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|+11318-01-01 00:00:00|COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130-01-01 00:00:00  |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530-01-01 00:00:00  |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911-01-01 00:00:00  |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657-01-01 00:00:00  |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648-01-01 00:00:00  |PENDING_PAYMENT|\n",
      "|11      |2013-07-25 00:00:00.0|NULL                 |PAYMENT_REVIEW |\n",
      "|12      |2013-07-25 00:00:00.0|1837-01-01 00:00:00  |CLOSED         |\n",
      "|13      |2013-07-25 00:00:00.0|9149-01-01 00:00:00  |PENDING_PAYMENT|\n",
      "|14      |2013-07-25 00:00:00.0|9842-01-01 00:00:00  |PROCESSING     |\n",
      "|15      |2013-07-25 00:00:00.0|2568-01-01 00:00:00  |COMPLETE       |\n",
      "|16      |2013-07-25 00:00:00.0|7276-01-01 00:00:00  |PENDING_PAYMENT|\n",
      "|17      |2013-07-25 00:00:00.0|2667-01-01 00:00:00  |COMPLETE       |\n",
      "|18      |2013-07-25 00:00:00.0|1205-01-01 00:00:00  |CLOSED         |\n",
      "|19      |2013-07-25 00:00:00.0|9488-01-01 00:00:00  |PENDING_PAYMENT|\n",
      "|20      |2013-07-25 00:00:00.0|9198-01-01 00:00:00  |PROCESSING     |\n",
      "+--------+---------------------+---------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
