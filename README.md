# ApacheSpark-CD â€” Ambiente de Estudo de Apache Spark (PySpark)

Este repositÃ³rio foi criado como um **laboratÃ³rio prÃ¡tico** para estudar **Apache Spark** (com foco em **PySpark**) num ambiente reproduzÃ­vel, fÃ¡cil de arrancar e consistente entre mÃ¡quinas.

O objetivo Ã© permitir:
- aprender Spark â€œa sÃ©rioâ€ (DataFrames, SQL, joins, window functions, otimizaÃ§Ã£o, partiÃ§Ãµes, etc.);
- praticar ingestÃ£o e transformaÃ§Ã£o de dados em batches;
- integrar Spark com uma base de dados relacional (**PostgreSQL**);
- trabalhar num ambiente padronizado via **Dev Containers / GitHub Codespaces**.

---

## âœ¨ O que vais encontrar aqui

- **Ambiente Docker/Devcontainer** para desenvolvimento rÃ¡pido e reproduzÃ­vel
- **PostgreSQL** como serviÃ§o (via Docker Compose)
- Scripts para:
  - preparar dependÃªncias Python e ambiente (`bootstrap.sh`)
  - obter datasets (`get_data.sh`)
  - inicializar/carregar SQL no Postgres (`run_all_sql_files.sh`)
- CÃ³digo PySpark para ETL/transformaÃ§Ãµes e escrita de outputs (ex.: Parquet)

> Nota: a estrutura exata pode variar, mas o repositÃ³rio estÃ¡ organizado para ser simples de executar e iterar.

---

## ğŸ§± PrÃ©-requisitos

### OpÃ§Ã£o A â€” Recomendado (mais simples)
- **GitHub Codespaces** (ou VS Code com Dev Containers)

### OpÃ§Ã£o B â€” Local
- Docker + Docker Compose
- VS Code + extensÃ£o *Dev Containers* (opcional, mas recomendado)

---

## ğŸš€ Quickstart (Codespaces / Devcontainer)

1. Abre o repositÃ³rio no GitHub
2. Clica em **Code â†’ Codespaces â†’ Create codespace**
3. O ambiente vai:
   - construir os containers (app + db)
   - configurar dependÃªncias e ambiente Python
   - (opcional) obter dados e preparar SQL dependendo dos scripts configurados

Dentro do Codespace, deves ter:
- workspace em: `/workspaces/ApacheSpark-CD`
- Python/venv em: `/workspaces/ApacheSpark-CD/.venv`
- Postgres acessÃ­vel pelo hostname: `db`

---

## ğŸ˜ PostgreSQL (via Docker Compose)

O serviÃ§o do Postgres Ã© levantado pelo Docker Compose e fica disponÃ­vel dentro do container `app` com:

- Host: `db`
- User: `postgres`
- Password: `postgres_password` (podes mudar)
- Porta: `5432`

> Se o repositÃ³rio for pÃºblico/partilhado, recomenda-se mover passwords para **Codespaces Secrets**.

---

## ğŸ§ª Fluxo tÃ­pico de estudo

### 1) Preparar ambiente Python
```bash
bash scripts/bootstrap.sh
````

### 2) Obter dados (se aplicÃ¡vel)

```bash
bash scripts/get_data.sh
```

### 3) Inicializar Postgres com SQL (se aplicÃ¡vel)

```bash
bash scripts/run_all_sql_files.sh
```

### 4) Correr um job Spark (exemplo)

> Ajusta o caminho conforme a estrutura do repositÃ³rio:

```bash
python jobs/exemplo_job.py
# ou
spark-submit jobs/exemplo_job.py
```

---

## ğŸ“š TÃ³picos que este repositÃ³rio pretende cobrir

### Spark / PySpark

* DataFrames: `select`, `filter`, `withColumn`, `groupBy`
* Joins: broadcast vs shuffle
* Window functions
* Leitura e escrita: CSV/Parquet
* PartiÃ§Ãµes, `repartition`, `coalesce`
* PersistÃªncia/caching
* Debug: DAG, stages, tasks (Spark UI)

### IntegraÃ§Ã£o com Postgres

* leitura via JDBC
* escrita via JDBC
* normalizaÃ§Ã£o de schemas
* carga incremental (quando aplicÃ¡vel)

### Boas prÃ¡ticas de engenharia

* ambiente reprodutÃ­vel (devcontainer)
* scripts idempotentes (evitar downloads/cargas repetidas)
* organizaÃ§Ã£o de jobs e outputs
* logging e observabilidade

---

## ğŸ—‚ï¸ Estrutura tÃ­pica do repositÃ³rio

> Exemplo (pode variar):

```
.
â”œâ”€ .devcontainer/
â”‚  â””â”€ devcontainer.json
â”œâ”€ docker-compose.yml
â”œâ”€ scripts/
â”‚  â”œâ”€ bootstrap.sh
â”‚  â”œâ”€ get_data.sh
â”‚  â””â”€ run_all_sql_files.sh
â”œâ”€ jobs/
â”‚  â””â”€ ...
â”œâ”€ data/
â”‚  â””â”€ ...
â””â”€ README.md
```

---

## ğŸ”§ Dicas para performance em Codespaces

* Considera **Prebuilds** para acelerar a criaÃ§Ã£o do ambiente
* Evita downloads pesados em `postCreateCommand` (melhor em `postStartCommand`)
* Torna `get_data.sh` e scripts SQL **idempotentes** (nÃ£o repetirem trabalho)

---

## ğŸ¤ ContribuiÃ§Ã£o

Este repositÃ³rio Ã© um ambiente de estudo. SugestÃµes e melhorias sÃ£o bem-vindas:

* issues com bugs/ideias
* PRs com novos jobs, datasets, exercÃ­cios e otimizaÃ§Ãµes



```bash
cd /workspaces/ApacheSpark-CD
tree -L 3
````
